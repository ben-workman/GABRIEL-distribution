{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39415dad",
   "metadata": {},
   "source": [
    "# GABRIEL Library Stress Test\n",
    "\n",
    "This notebook exercises various API calls and pipelines in dummy mode to verify the library works as expected. All OpenAI calls use the built-in dummy responses, so it can run offline and be re-run safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e35f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, shutil, asyncio, pandas as pd\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "from gabriel.utils import openai_utils\n",
    "from gabriel.tasks import (\n",
    "    Ratings, RatingsConfig,\n",
    "    BasicClassifier, BasicClassifierConfig,\n",
    "    Deidentifier, DeidentifyConfig,\n",
    "    Regional, RegionalConfig,\n",
    "    CountyCounter,\n",
    "    EloRater, EloConfig,\n",
    "    RecursiveEloConfig, RecursiveEloRater,\n",
    ")\n",
    "from gabriel.utils import Teleprompter, PromptParaphraser, PromptParaphraserConfig\n",
    "\n",
    "out_dir = 'stress_test_outputs'\n",
    "if os.path.exists(out_dir):\n",
    "    shutil.rmtree(out_dir)\n",
    "os.makedirs(out_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\ntry:\n    from sklearn.datasets import fetch_20newsgroups\n    ng = fetch_20newsgroups(subset='train', categories=['sci.space'], remove=('headers','footers','quotes'), download_if_missing=False)\n    sample_texts = ng.data[:3]\nexcept Exception:\n    sample_texts = ['Space exploration','Galaxy news','Astronomy facts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\nng_df = await openai_utils.get_all_responses(prompts=sample_texts, identifiers=['ng1','ng2','ng3'], use_dummy=True, save_path=os.path.join(out_dir,'ng.csv'))\nng_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff850e82",
   "metadata": {},
   "source": [
    "## Basic `get_all_responses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\nprompts = ['Hello world', 'How are you?']\ndf = await openai_utils.get_all_responses(prompts=prompts, identifiers=['p1','p2'], use_dummy=True, save_path=os.path.join(out_dir,'basic.csv'))\ndf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edcbda",
   "metadata": {},
   "source": [
    "## JSON mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\njson_prompts = ['{\"a\":1}', '{\"b\":2}']\nschema = {\"type\": \"object\"}\njson_df = await openai_utils.get_all_responses(prompts=json_prompts, json_mode=True, expected_schema=schema, use_dummy=True, save_path=os.path.join(out_dir,'json.csv'))\njson_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e706606",
   "metadata": {},
   "source": [
    "## Web search tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d025ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\nsearch_prompts = ['What is the capital of France?']\nweb_df = await openai_utils.get_all_responses(prompts=search_prompts, identifiers=['search'], use_web_search=True, use_dummy=True, save_path=os.path.join(out_dir,'web.csv'))\nweb_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5299f5",
   "metadata": {},
   "source": [
    "## Resume from existing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05872a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\nresume_prompts = ['A1','A2','A3']\n# First run with only two prompts\n_ = await openai_utils.get_all_responses(prompts=resume_prompts[:2], identifiers=['r1','r2'], use_dummy=True, save_path=os.path.join(out_dir,'resume.csv'))\n# Second run with all prompts (should only process missing one)\nresume_df = await openai_utils.get_all_responses(prompts=resume_prompts, identifiers=['r1','r2','r3'], use_dummy=True, save_path=os.path.join(out_dir,'resume.csv'))\nresume_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19995ae0",
   "metadata": {},
   "source": [
    "## Ratings pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\ndata = pd.DataFrame({'text': ['This product is great.', 'Terrible experience.']})\nratings_cfg = RatingsConfig(attributes={'quality':'Overall quality'}, save_dir=os.path.join(out_dir,'ratings'), use_dummy=True)\nratings_res = await Ratings(ratings_cfg).run(data, text_column='text')\nratings_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3678419d",
   "metadata": {},
   "source": [
    "## BasicClassifier pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc292dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\nclf_data = pd.DataFrame({'txt': ['I love pizza', 'I hate spinach']})\nclf_cfg = BasicClassifierConfig(labels={'positive':'Is the sentiment positive?'}, save_dir=os.path.join(out_dir,'classifier'), use_dummy=True)\nclf_res = await BasicClassifier(clf_cfg).run(clf_data, text_column='txt')\nclf_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d3574",
   "metadata": {},
   "source": [
    "## Deidentifier pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\ndeid_data = pd.DataFrame({'text':['John Doe went to New York.']})\ndeid_cfg = DeidentifyConfig(save_path=os.path.join(out_dir,'deid.csv'), use_dummy=True)\ndeid_res = await Deidentifier(deid_cfg).run(deid_data, text_column='text')\ndeid_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f2a56",
   "metadata": {},
   "source": [
    "## Regional analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9227cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\nreg_data = pd.DataFrame({'county':['A','B']})\nreg_cfg = RegionalConfig(save_dir=os.path.join(out_dir,'regional'), use_dummy=True)\nregional_task = Regional(reg_data, 'county', topics=['economy'], cfg=reg_cfg)\nregional_res = await regional_task.run()\nregional_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c9172",
   "metadata": {},
   "source": [
    "## CountyCounter pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109dc797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\ncounty_data = pd.DataFrame({'county':['A','B'], 'fips':['00001','00002']})\ncc = CountyCounter(county_data, county_col='county', topics=['econ'], fips_col='fips', save_dir=os.path.join(out_dir,'county'), use_dummy=True, n_elo_rounds=1)\ncounty_res = await cc.run()\ncounty_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b8aff",
   "metadata": {},
   "source": [
    "## EloRater pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d35971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\nelo_data = pd.DataFrame({'identifier':['x','y'], 'text':['Text X','Text Y']})\ntele = Teleprompter()\nelo_cfg = EloConfig(attributes={'clarity':''}, n_rounds=1, save_dir=os.path.join(out_dir,'elo'), use_dummy=True)\nelo_task = EloRater(tele, elo_cfg)\nelo_res = await elo_task.run(elo_data, text_col='text', id_col='identifier')\nelo_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a8a7fd",
   "metadata": {},
   "source": [
    "## RecursiveEloRater pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9886a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\nrec_data = pd.DataFrame({'identifier':['a','b','c'], 'text':['Alpha','Bravo','Charlie']})\nbase_cfg = EloConfig(attributes={'score':''}, n_rounds=1, save_dir=os.path.join(out_dir,'rec'), use_dummy=True)\nrec_cfg = RecursiveEloConfig(base_cfg=base_cfg, min_remaining=2)\nrec_task = RecursiveEloRater(tele, rec_cfg)\nrec_res = await rec_task.run(rec_data, text_col='text', id_col='identifier')\nrec_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\nfrom gabriel.utils.parsing import safest_json\nfrom unittest.mock import patch\n\nasync def fake_response(*args, **kwargs):\n    return ['{\"good\": true}'], 0.0\n\nwith patch('gabriel.utils.openai_utils.get_response', fake_response):\n    fixed_json = await safest_json('{bad:1}')\nfixed_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a8d05",
   "metadata": {},
   "source": [
    "## PromptParaphraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\nbatch_prompts = [f'B{i}' for i in range(4)]\nbatch_df = await openai_utils.get_all_responses(\n    prompts=batch_prompts,\n    identifiers=[f'b{i}' for i in range(4)],\n    use_batch=True,\n    use_dummy=True,\n    save_path=os.path.join(out_dir,'batch.csv'),\n)\nbatch_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\nparap_cfg = PromptParaphraserConfig(n_variants=2, save_dir=os.path.join(out_dir,'parap'), use_dummy=True)\nparap = PromptParaphraser(parap_cfg)\nparap_res = await parap.run(Ratings, ratings_cfg, data, text_column='text')\nparap_res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}